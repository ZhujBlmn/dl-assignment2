#!/usr/bin/env python3
import argparse
import json
import time
import os
import torch
import torchaudio
import librosa
import numpy as np
from tqdm import tqdm
from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq
from cosyvoice.cli.cosyvoice import CosyVoice
from concurrent.futures import ThreadPoolExecutor, as_completed

# --- huggingface_hub compatibility patch (for CosyVoice) ---
try:
    import huggingface_hub as _hfh
    if not hasattr(_hfh, "cached_download"):
        from huggingface_hub import hf_hub_download as _hf_hub_download

        def cached_download(*args, **kwargs):
            return _hf_hub_download(*args, **kwargs)

        _hfh.cached_download = cached_download
except Exception:
    pass


def load_jsonl(path):
    """å¿«é€ŸåŠ è½½JSONLæ–‡ä»¶"""
    items = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            items.append(json.loads(line))
    return items


def fast_load_audio_batch(audio_paths, target_sr=16000, max_workers=4):
    """æ‰¹é‡å¿«é€ŸåŠ è½½éŸ³é¢‘æ–‡ä»¶ï¼ˆä½¿ç”¨å¤šçº¿ç¨‹ï¼‰"""
    def load_single_audio(path):
        try:
            # ä½¿ç”¨librosaåŠ è½½ï¼Œé€šå¸¸æ¯”torchaudioå¿«
            audio, sr = librosa.load(path, sr=target_sr, mono=True)
            return torch.from_numpy(audio).float(), path
        except Exception as e:
            print(f"Error loading {path}: {e}")
            return None, path
    
    results = {}
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_path = {executor.submit(load_single_audio, path): path for path in audio_paths}
        for future in tqdm(as_completed(future_to_path), total=len(audio_paths), desc="Loading audio"):
            result, path = future.result()
            if result is not None:
                results[path] = result
    return results


def extract_whisper_encoder_feats_batch(batch_audio, model, processor, device, max_duration=30.0):
    """æ‰¹é‡æå–Whisperç‰¹å¾ï¼ˆæ˜¾è‘—æå‡é€Ÿåº¦ï¼‰"""
    batch_results = {}
    
    # è¿‡æ»¤è¶…é•¿éŸ³é¢‘
    valid_audio = {}
    for path, waveform in batch_audio.items():
        num_seconds = waveform.numel() / 16000.0
        if num_seconds <= max_duration:
            valid_audio[path] = waveform
    
    if not valid_audio:
        return batch_results
    
    # æ‰¹é‡å¤„ç†
    audio_paths = list(valid_audio.keys())
    waveforms = list(valid_audio.values())
    
    # å°†éŸ³é¢‘æ•°æ®è½¬æ¢ä¸ºnumpyæ•°ç»„
    audio_arrays = [waveform.cpu().numpy() for waveform in waveforms]
    
    # æ‰¹é‡å¤„ç†ï¼ˆWhisperå¤„ç†å™¨æ”¯æŒæ‰¹é‡å¤„ç†ï¼‰
    inputs = processor(
        audio_arrays,
        sampling_rate=16000,
        return_tensors="pt",
        padding=True,
        return_attention_mask=True
    )
    
    input_features = inputs.input_features.to(device)
    attention_mask = inputs.attention_mask.to(device) if inputs.attention_mask is not None else None
    
    with torch.no_grad():
        enc_out = model.model.encoder(
            input_features=input_features,
            attention_mask=attention_mask,
            output_hidden_states=True,
        )
    
    hidden_states = enc_out.hidden_states
    mid_idx = len(hidden_states) // 2
    
    # å¤„ç†æ¯ä¸ªæ ·æœ¬çš„ç»“æœ
    for i, path in enumerate(audio_paths):
        # æ ¹æ®attention_maskè·å–æœ‰æ•ˆé•¿åº¦
        if attention_mask is not None:
            valid_length = attention_mask[i].sum().item()
            mid_layer = hidden_states[mid_idx][i, :valid_length].cpu()
            final_layer = hidden_states[-1][i, :valid_length].cpu()
        else:
            mid_layer = hidden_states[mid_idx][i].cpu().squeeze(0)
            final_layer = hidden_states[-1][i].cpu().squeeze(0)
        
        batch_results[path] = (mid_layer, final_layer)
    
    return batch_results


def extract_text_embeddings_batch(texts_with_paths, cosy, device, batch_size=32):
    """æ‰¹é‡æå–æ–‡æœ¬åµŒå…¥"""
    text_embeddings = {}
    
    # åˆ†æ‰¹å¤„ç†æ–‡æœ¬
    for i in range(0, len(texts_with_paths), batch_size):
        batch_items = texts_with_paths[i:i + batch_size]
        batch_texts = [item[1] for item in batch_items]
        batch_paths = [item[0] for item in batch_items]
        
        # æ‰¹é‡æå–æ–‡æœ¬token
        batch_tokens = []
        batch_token_lens = []
        
        for text in batch_texts:
            text_token, text_token_len = cosy.frontend._extract_text_token(text)
            batch_tokens.append(text_token)
            batch_token_lens.append(text_token_len)
        
        # å †å tokens
        max_len = max(token.size(1) for token in batch_tokens)
        padded_tokens = []
        
        for token in batch_tokens:
            pad_size = max_len - token.size(1)
            if pad_size > 0:
                padded_token = torch.cat([
                    token, 
                    torch.zeros((1, pad_size), dtype=token.dtype, device=token.device)
                ], dim=1)
            else:
                padded_token = token
            padded_tokens.append(padded_token)
        
        stacked_tokens = torch.cat(padded_tokens, dim=0).to(device)
        
        # æ‰¹é‡è®¡ç®—åµŒå…¥
        with torch.no_grad():
            batch_embeddings = cosy.model.llm.text_embedding(stacked_tokens)
        
        # ä¿å­˜ç»“æœï¼ˆå»é™¤paddingï¼‰
        for j, (path, original_token) in enumerate(zip(batch_paths, batch_tokens)):
            original_len = original_token.size(1)
            text_embedding = batch_embeddings[j, :original_len].cpu()
            text_embeddings[path] = text_embedding
    
    return text_embeddings


def main(args):
    print("ğŸš€ å¼€å§‹ä¼˜åŒ–å¤„ç†...")
    start_time = time.time()
    
    # è®¾ç½®è®¾å¤‡
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ“Š ä½¿ç”¨è®¾å¤‡: {device}")
    if device == "cuda":
        print(f"ğŸ® GPU: {torch.cuda.get_device_name(0)}")
        print(f"ğŸ’¾ æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    
    # åŠ è½½æ¨¡å‹
    print("ğŸ”„ åŠ è½½æ¨¡å‹ä¸­...")
    model_load_start = time.time()
    
    # ä½¿ç”¨æ›´å°çš„Whisperæ¨¡å‹ä»¥åŠ é€Ÿï¼ˆå¯ä»¥æ”¹ä¸ºbase/smallï¼‰
    whisper_model_name = "openai/whisper-base"  # æ¯”large-v3å¿«å¾ˆå¤š
    if args.fast_mode:
        whisper_model_name = "openai/whisper-small"  # æé€Ÿæ¨¡å¼
    
    cosy = CosyVoice(args.model_dir)
    processor = AutoProcessor.from_pretrained(whisper_model_name)
    whisper_model = AutoModelForSpeechSeq2Seq.from_pretrained(whisper_model_name).to(device)
    whisper_model.eval()
    
    # å¯ç”¨torch2.0ç¼–è¯‘ä¼˜åŒ–ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if hasattr(torch, 'compile') and device == "cuda":
        print("âš¡ å¯ç”¨Torchç¼–è¯‘ä¼˜åŒ–...")
        whisper_model = torch.compile(whisper_model, mode="reduce-overhead")
    
    print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ: {time.time() - model_load_start:.2f}s")
    
    # åŠ è½½æ•°æ®
    data = load_jsonl(args.jsonl)
    print(f"ğŸ“ åŠ è½½ {len(data)} ä¸ªæ ·æœ¬")
    
    # å‡†å¤‡æ‰¹é‡æ•°æ®
    audio_paths = [item["audio_path"] for item in data]
    texts_with_paths = [(item["audio_path"], item["text"]) for item in data]
    
    # é˜¶æ®µ1: æ‰¹é‡åŠ è½½éŸ³é¢‘ï¼ˆå¤šçº¿ç¨‹ï¼‰
    print("ğŸµ æ‰¹é‡åŠ è½½éŸ³é¢‘æ–‡ä»¶...")
    audio_loading_start = time.time()
    audio_data = fast_load_audio_batch(audio_paths, max_workers=args.num_workers)
    print(f"âœ… éŸ³é¢‘åŠ è½½å®Œæˆ: {time.time() - audio_loading_start:.2f}s")
    
    # é˜¶æ®µ2: æ‰¹é‡å¤„ç†æ–‡æœ¬åµŒå…¥
    print("ğŸ“ æ‰¹é‡å¤„ç†æ–‡æœ¬åµŒå…¥...")
    text_embedding_start = time.time()
    utt2text_emb = extract_text_embeddings_batch(
        texts_with_paths, cosy, device, batch_size=args.batch_size
    )
    print(f"âœ… æ–‡æœ¬åµŒå…¥å®Œæˆ: {time.time() - text_embedding_start:.2f}s")
    
    # é˜¶æ®µ3: æ‰¹é‡å¤„ç†Whisperç‰¹å¾
    print("ğŸ¤ æ‰¹é‡å¤„ç†Whisperç‰¹å¾...")
    whisper_start = time.time()
    
    # åˆ†æ‰¹å¤„ç†éŸ³é¢‘ä»¥é¿å…OOM
    batch_size = min(args.batch_size, 8)  # Whisperæ‰¹å¤„ç†è¾ƒå°ä»¥é¿å…å†…å­˜æº¢å‡º
    utt2whisper_mid = {}
    utt2whisper_final = {}
    
    audio_items = list(audio_data.items())
    for i in range(0, len(audio_items), batch_size):
        batch_items = audio_items[i:i + batch_size]
        batch_dict = dict(batch_items)
        
        batch_results = extract_whisper_encoder_feats_batch(
            batch_dict, whisper_model, processor, device, args.max_duration
        )
        
        for path, (mid_feat, final_feat) in batch_results.items():
            utt2whisper_mid[path] = mid_feat
            utt2whisper_final[path] = final_feat
    
    print(f"âœ… Whisperç‰¹å¾æå–å®Œæˆ: {time.time() - whisper_start:.2f}s")
    
    # ä¿å­˜ç»“æœ
    print("ğŸ’¾ ä¿å­˜ç»“æœ...")
    torch.save(utt2text_emb, args.output_text)
    
    whisper_output = {
        "mid": utt2whisper_mid,
        "final": utt2whisper_final,
    }
    torch.save(whisper_output, args.output_whisper)
    
    total_time = time.time() - start_time
    print(f"ğŸ‰ å¤„ç†å®Œæˆ!")
    print(f"â±ï¸  æ€»è€—æ—¶: {total_time:.2f}s ({total_time/60:.1f}åˆ†é’Ÿ)")
    print(f"ğŸ“Š å¤„ç†é€Ÿåº¦: {len(data)/total_time:.2f} æ ·æœ¬/ç§’")
    print(f"ğŸ’¾ æ–‡æœ¬åµŒå…¥ä¿å­˜è‡³: {args.output_text}")
    print(f"ğŸ’¾ Whisperç‰¹å¾ä¿å­˜è‡³: {args.output_whisper}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="ä¼˜åŒ–ç‰ˆç‰¹å¾æå–è„šæœ¬")
    parser.add_argument("--jsonl", type=str, required=True, help="Input jsonl with audio_path and text")
    parser.add_argument("--model_dir", type=str, required=True, help="CosyVoice1 model dir")
    parser.add_argument("--output_text", type=str, required=True, help="Output .pt for CosyVoice text embeddings")
    parser.add_argument("--output_whisper", type=str, required=True, help="Output .pt for Whisper features")
    parser.add_argument("--max_duration", type=float, default=30.0, help="Max audio length (seconds) to process")
    parser.add_argument("--batch_size", type=int, default=16, help="Batch size for processing")
    parser.add_argument("--num_workers", type=int, default=4, help="Number of workers for audio loading")
    parser.add_argument("--fast_mode", action="store_true", help="Use smaller models for maximum speed")
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    os.makedirs(os.path.dirname(args.output_text) if os.path.dirname(args.output_text) else ".", exist_ok=True)
    os.makedirs(os.path.dirname(args.output_whisper) if os.path.dirname(args.output_whisper) else ".", exist_ok=True)
    
    main(args)